{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Human Activities - Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour ce TP nous allons mettre en pratique plusieurs méthodes de classification supervisée sur des données d'enregistrement d'acceleromètres (de smart-phones).\n",
    "\n",
    "Les données sources sont disponibles sur [ici](http://bertrand.michel.perso.math.cnrs.fr/Enseignements/Data/UCI-HAR-Dataset.zip), voir aussi la description sur le site de [la plateforme de données UCI](https://archive.ics.uci.edu/ml/datasets/human+activity+recognition+using+smartphones).\n",
    "\n",
    "\"The experiments have been carried out with a group of 30 volunteers within an age bracket of 19-48 years. Each person performed six activities (WALKING, WALKING_UPSTAIRS, WALKING_DOWNSTAIRS, SITTING, STANDING, LAYING) wearing a smartphone (Samsung Galaxy S II) on the waist. Using its embedded accelerometer and gyroscope, we captured 3-axial linear acceleration and 3-axial angular velocity at a constant rate of 50Hz. The experiments have been video-recorded to label the data manually.\n",
    "\n",
    "The sensor signals (accelerometer and gyroscope) were pre-processed by applying noise filters and then sampled in fixed-width sliding windows of 2.56 sec and 50% overlap (128 readings/window). The sensor acceleration signal, which has gravitational and body motion components, was separated using a Butterworth low-pass filter into body acceleration and gravity. The gravitational force is assumed to have only low frequency components, therefore a filter with 0.3 Hz cutoff frequency was used. From each window, a vector of features was obtained by calculating variables from the time and frequency domain. See 'features_info.txt' for more details.\"\n",
    "\n",
    "L'objectif de ce TP est de retrouver l'activité à partir de toutes ces descripteurs (features). Notez que dans un contexte plus réaliste il vous faudrait créer par vosu même tous ces features qui décrivent les séries temporelles des accelerations enregistrées.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from pylab import *\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importation des données\n",
    "\n",
    "\n",
    "Les commandes suivantes permettent de récupérer les données.\n",
    "Pour simplifier on n'utilise pour le TP que les données du dossier \"train\".\n",
    "\n",
    "> Vérifiez que vous comprenez l'ensemble des commandes ci-dessous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "votre_path= \"/home/rlucas/Documents/STASC/UCI-HAR-Dataset/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importation des features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = votre_path + \"train/X_train.txt\"\n",
    "activity_features = pd.read_csv(data_path,delim_whitespace=True,\n",
    "                                header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importation des activités :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 4 4 ... 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "data_path = votre_path + \"train/y_train.txt\"\n",
    "activity  =    pd.read_csv(data_path,delim_whitespace=True,header=None)\n",
    "activity  =  activity.values[:,0] -  1 \n",
    "# la première activité sera 0 (plus pratique en python)\n",
    "activity_names = ['WALKING','WALKING_UPSTAIRS','WALKING_DOWNSTAIRS','SITTING','STANDING','LAYING']\n",
    "print(activity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La variable à prédire  est la variable d'activité. Il s'agit donc d'un problème de classification (à 6 classes).\n",
    "Pour cela on dispose des variables du tableau `activity_features`. \n",
    "> Combien de variables sont disponibles pour construire le prédicteur de l'activité ?  De combien d'observations dispose-t-on ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>551</th>\n",
       "      <th>552</th>\n",
       "      <th>553</th>\n",
       "      <th>554</th>\n",
       "      <th>555</th>\n",
       "      <th>556</th>\n",
       "      <th>557</th>\n",
       "      <th>558</th>\n",
       "      <th>559</th>\n",
       "      <th>560</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "      <td>7352.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.274488</td>\n",
       "      <td>-0.017695</td>\n",
       "      <td>-0.109141</td>\n",
       "      <td>-0.605438</td>\n",
       "      <td>-0.510938</td>\n",
       "      <td>-0.604754</td>\n",
       "      <td>-0.630512</td>\n",
       "      <td>-0.526907</td>\n",
       "      <td>-0.606150</td>\n",
       "      <td>-0.468604</td>\n",
       "      <td>...</td>\n",
       "      <td>0.125293</td>\n",
       "      <td>-0.307009</td>\n",
       "      <td>-0.625294</td>\n",
       "      <td>0.008684</td>\n",
       "      <td>0.002186</td>\n",
       "      <td>0.008726</td>\n",
       "      <td>-0.005981</td>\n",
       "      <td>-0.489547</td>\n",
       "      <td>0.058593</td>\n",
       "      <td>-0.056515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.070261</td>\n",
       "      <td>0.040811</td>\n",
       "      <td>0.056635</td>\n",
       "      <td>0.448734</td>\n",
       "      <td>0.502645</td>\n",
       "      <td>0.418687</td>\n",
       "      <td>0.424073</td>\n",
       "      <td>0.485942</td>\n",
       "      <td>0.414122</td>\n",
       "      <td>0.544547</td>\n",
       "      <td>...</td>\n",
       "      <td>0.250994</td>\n",
       "      <td>0.321011</td>\n",
       "      <td>0.307584</td>\n",
       "      <td>0.336787</td>\n",
       "      <td>0.448306</td>\n",
       "      <td>0.608303</td>\n",
       "      <td>0.477975</td>\n",
       "      <td>0.511807</td>\n",
       "      <td>0.297480</td>\n",
       "      <td>0.279122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.999873</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.995357</td>\n",
       "      <td>-0.999765</td>\n",
       "      <td>-0.976580</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.262975</td>\n",
       "      <td>-0.024863</td>\n",
       "      <td>-0.120993</td>\n",
       "      <td>-0.992754</td>\n",
       "      <td>-0.978129</td>\n",
       "      <td>-0.980233</td>\n",
       "      <td>-0.993591</td>\n",
       "      <td>-0.978162</td>\n",
       "      <td>-0.980251</td>\n",
       "      <td>-0.936219</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023692</td>\n",
       "      <td>-0.542602</td>\n",
       "      <td>-0.845573</td>\n",
       "      <td>-0.121527</td>\n",
       "      <td>-0.289549</td>\n",
       "      <td>-0.482273</td>\n",
       "      <td>-0.376341</td>\n",
       "      <td>-0.812065</td>\n",
       "      <td>-0.017885</td>\n",
       "      <td>-0.143414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.277193</td>\n",
       "      <td>-0.017219</td>\n",
       "      <td>-0.108676</td>\n",
       "      <td>-0.946196</td>\n",
       "      <td>-0.851897</td>\n",
       "      <td>-0.859365</td>\n",
       "      <td>-0.950709</td>\n",
       "      <td>-0.857328</td>\n",
       "      <td>-0.857143</td>\n",
       "      <td>-0.881637</td>\n",
       "      <td>...</td>\n",
       "      <td>0.134000</td>\n",
       "      <td>-0.343685</td>\n",
       "      <td>-0.711692</td>\n",
       "      <td>0.009509</td>\n",
       "      <td>0.008943</td>\n",
       "      <td>0.008735</td>\n",
       "      <td>-0.000368</td>\n",
       "      <td>-0.709417</td>\n",
       "      <td>0.182071</td>\n",
       "      <td>0.003181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.288461</td>\n",
       "      <td>-0.010783</td>\n",
       "      <td>-0.097794</td>\n",
       "      <td>-0.242813</td>\n",
       "      <td>-0.034231</td>\n",
       "      <td>-0.262415</td>\n",
       "      <td>-0.292680</td>\n",
       "      <td>-0.066701</td>\n",
       "      <td>-0.265671</td>\n",
       "      <td>-0.017129</td>\n",
       "      <td>...</td>\n",
       "      <td>0.289096</td>\n",
       "      <td>-0.126979</td>\n",
       "      <td>-0.503878</td>\n",
       "      <td>0.150865</td>\n",
       "      <td>0.292861</td>\n",
       "      <td>0.506187</td>\n",
       "      <td>0.359368</td>\n",
       "      <td>-0.509079</td>\n",
       "      <td>0.248353</td>\n",
       "      <td>0.107659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.916238</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.967664</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.946700</td>\n",
       "      <td>0.989538</td>\n",
       "      <td>0.956845</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998702</td>\n",
       "      <td>0.996078</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.478157</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 561 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0            1            2            3            4    \\\n",
       "count  7352.000000  7352.000000  7352.000000  7352.000000  7352.000000   \n",
       "mean      0.274488    -0.017695    -0.109141    -0.605438    -0.510938   \n",
       "std       0.070261     0.040811     0.056635     0.448734     0.502645   \n",
       "min      -1.000000    -1.000000    -1.000000    -1.000000    -0.999873   \n",
       "25%       0.262975    -0.024863    -0.120993    -0.992754    -0.978129   \n",
       "50%       0.277193    -0.017219    -0.108676    -0.946196    -0.851897   \n",
       "75%       0.288461    -0.010783    -0.097794    -0.242813    -0.034231   \n",
       "max       1.000000     1.000000     1.000000     1.000000     0.916238   \n",
       "\n",
       "               5            6            7            8            9    ...  \\\n",
       "count  7352.000000  7352.000000  7352.000000  7352.000000  7352.000000  ...   \n",
       "mean     -0.604754    -0.630512    -0.526907    -0.606150    -0.468604  ...   \n",
       "std       0.418687     0.424073     0.485942     0.414122     0.544547  ...   \n",
       "min      -1.000000    -1.000000    -1.000000    -1.000000    -1.000000  ...   \n",
       "25%      -0.980233    -0.993591    -0.978162    -0.980251    -0.936219  ...   \n",
       "50%      -0.859365    -0.950709    -0.857328    -0.857143    -0.881637  ...   \n",
       "75%      -0.262415    -0.292680    -0.066701    -0.265671    -0.017129  ...   \n",
       "max       1.000000     1.000000     0.967664     1.000000     1.000000  ...   \n",
       "\n",
       "               551          552          553          554          555  \\\n",
       "count  7352.000000  7352.000000  7352.000000  7352.000000  7352.000000   \n",
       "mean      0.125293    -0.307009    -0.625294     0.008684     0.002186   \n",
       "std       0.250994     0.321011     0.307584     0.336787     0.448306   \n",
       "min      -1.000000    -0.995357    -0.999765    -0.976580    -1.000000   \n",
       "25%      -0.023692    -0.542602    -0.845573    -0.121527    -0.289549   \n",
       "50%       0.134000    -0.343685    -0.711692     0.009509     0.008943   \n",
       "75%       0.289096    -0.126979    -0.503878     0.150865     0.292861   \n",
       "max       0.946700     0.989538     0.956845     1.000000     1.000000   \n",
       "\n",
       "               556          557          558          559          560  \n",
       "count  7352.000000  7352.000000  7352.000000  7352.000000  7352.000000  \n",
       "mean      0.008726    -0.005981    -0.489547     0.058593    -0.056515  \n",
       "std       0.608303     0.477975     0.511807     0.297480     0.279122  \n",
       "min      -1.000000    -1.000000    -1.000000    -1.000000    -1.000000  \n",
       "25%      -0.482273    -0.376341    -0.812065    -0.017885    -0.143414  \n",
       "50%       0.008735    -0.000368    -0.709417     0.182071     0.003181  \n",
       "75%       0.506187     0.359368    -0.509079     0.248353     0.107659  \n",
       "max       0.998702     0.996078     1.000000     0.478157     1.000000  \n",
       "\n",
       "[8 rows x 561 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activity_features.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7352"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(activity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour de nombreux méthodes d'appprentissage statistique, il est conseiller de préalablement standardiser les données."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Effectuer une standardisation des features (activity_features) avec la fonction  [scale()](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.scale.html) de scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_nor = preprocessing.scale(activity_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importation des sujets :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = votre_path + \"train/subject_train.txt\"\n",
    "sujet =  pd.read_csv(data_path,delim_whitespace=True,header=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A chaque sujet correspond plusieurs observations.\n",
    "> Indiquer le nombre d'observations de chaque sujet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25    409\n",
      "21    408\n",
      "26    392\n",
      "30    383\n",
      "28    382\n",
      "27    376\n",
      "23    372\n",
      "17    368\n",
      "16    366\n",
      "19    360\n",
      "1     347\n",
      "29    344\n",
      "3     341\n",
      "15    328\n",
      "6     325\n",
      "14    323\n",
      "22    321\n",
      "11    316\n",
      "7     308\n",
      "5     302\n",
      "8     281\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(sujet.value_counts())\n",
    "sujet =sujet.values[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problème de classification à deux classes\n",
    "\n",
    "Dans toute la première partie du TP, on considère un problème de classification à deux classes. \n",
    "\n",
    "> Extraire les donnnés pour les activités  'WALKING_DOWNSTAIRS' (activity = 3)  et 'SITTING' (activity =4) uniquement. On appele `features34` et `activity34` les données correspondantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2660, 561)\n"
     ]
    }
   ],
   "source": [
    "n,p = shape(features_nor)\n",
    "index_act34 = [i for i in range(n) if activity[i] in [3,4]] \n",
    "activity34 = np.array([activity[i] for i in index_act34])\n",
    "features34 = np.array([activity_features.values[i] for i in index_act34])\n",
    "print(shape(features34))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Présélection de features  pour réduire la dimension (screening)\n",
    "\n",
    "Pour diminuer les temps de calul on peut pré-sélectionner les 100 features les plus discriminants via un critère univarié. \n",
    "\n",
    "> Utiliser le critère  [SelectKBest](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html) basé sur la F-value (variance inter / variance intra).\n",
    "\n",
    "Attention, cette méthode ne garantit en rien que l'on a choisi le \"meilleur\" groupe de 100 variables pour le problème de classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2660, 100)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "Selec = SelectKBest(f_classif, k = 100)\n",
    "features34 = Selec.fit_transform(features34, activity34)\n",
    "shape(features34)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ajustement et prédiction pour des méthodes classiques de classification \n",
    "\n",
    "> Pour chacune des méthodes de classificiation listées ci-dessous, ajuster un predicteur de l'activité en utilisant comme données d'apprentissage la base (features34,activity34) :      \n",
    "- Classifieur naif bayesien avec la fonction `GaussianNB()`   \n",
    "- Classifieur des k plus proches voisins avec `neighbors.KNeighborsClassifier()` avec 5 plus proches voisins.  \n",
    "- Classifieur par régression logistique avec pénalisation ridge, en utilisant la fonction `linear_model.LogisticRegression() `. Indiquer la pénalité \"l2\" dans les arguments de la fonction et choisir le solveur \"saga\" (voir la doc de la fonction).\n",
    "\n",
    "\n",
    "Une présentation de chacune des ces fonctions est disponible sur cette [page](http://scikit-learn.org/stable/user_guide.html).\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rlucas/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(solver='saga')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB \n",
    "gnb = GaussianNB()\n",
    "gnb.fit(features34, activity34)\n",
    "\n",
    "from sklearn import neighbors\n",
    "nn = neighbors.KNeighborsClassifier()\n",
    "nn.fit(features34, activity34)\n",
    "\n",
    "\n",
    "from sklearn import linear_model\n",
    "logit =  linear_model.LogisticRegression(penalty = 'l2', solver = 'saga')\n",
    "logit.fit(features34, activity34)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour chacune de ces méthodes, on peut faire une prédiction grâce à la méthode `predict()`. par exemple pour le classifieur naif bayesien : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 3 4 ... 3 3 3]\n"
     ]
    }
   ],
   "source": [
    "feat_pred_gnb = gnb.predict(features34)\n",
    "print(feat_pred_gnb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Faire de même une prédiction pour le classifieur des plus proches voisins et pour la régression logistique. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 4 4 ... 3 3 3]\n",
      "[4 4 4 ... 3 3 3]\n"
     ]
    }
   ],
   "source": [
    "feat_pred_nn = nn.predict(features34)\n",
    "print(feat_pred_nn)\n",
    "\n",
    "feat_pred_logit = logit.predict(features34)\n",
    "print(feat_pred_logit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Ces prédictions sont-elles cohérentes entre elles ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les prédictions ne semblent pas toutes d'accord. Il y a quelques imprécisio du côté de gnb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Pour chaque prédicteur :\n",
    "> - Calculer le taux d'erreur \"à la main\" en comparant les observations et les prédictions.\n",
    "> - Calculer le taux de bon classement avec la méthode `score()` (disponible pour tout predicteur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORES\n",
      "##########\n",
      "gnb:\t0.8056390977443609\n",
      "nn:\t0.9522556390977444\n",
      "logit:\t0.956015037593985\n"
     ]
    }
   ],
   "source": [
    "### TO DO: comparaison à la main\n",
    "print(\"SCORES\")\n",
    "print(\"##########\")\n",
    "print(f\"gnb:\\t{gnb.score(features34, activity34)}\")\n",
    "print(f\"nn:\\t{nn.score(features34, activity34)}\")\n",
    "print(f\"logit:\\t{logit.score(features34, activity34)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evidemment ce n'est pas pas la bonne façon d'estimer le risque de l'estimateur : estimer de l'erreur de généralisation en évaluant les erreurs sur l'échantillon qui a déjà servi à ajuster le prédicteur conduit la plupart du temps à une estimation trop optimiste."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation de l'erreur par découpage train / test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons maintenant évaluer l'erreur de généralisation en conservant une partie de l'échantillon pour évaluer les erreurs du prédicteur ajusté. Pour cela on découpe aléatoirement l'échantillon initial en deux parties :  \n",
    "\n",
    "- l'ensemble d'apprentissage : utilisé pour ajuster les prédicteurs ;\n",
    "- l'ensemble de test (ou de validation) : utilisé pour évaluer les performances des prédicteurs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Utiliser la fonction <code>train_test_split</code> du module [<code>model_selection</code>](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.model_selection) pour construire un échantillon d'apprentissage de taille 60% (et donc 40% réservées aux données de test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0729323308270677\n",
      "0.6909774436090226\n",
      "(1838, 561)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import model_selection\n",
    "\n",
    "features_train, features_test, activity_train, activity_test = model_selection.train_test_split(activity_features, activity)\n",
    "print(len(activity_train)/ float(len(activity34)))\n",
    "print(len(activity_test)/ float(len(activity34)))\n",
    "print(shape(features_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Evaluer le risque d'un classifieur par plus proches voisins (knn) avec 5 voisins sur ce découpage des données : ajuster sur les données d'apprentissage et évaluer les erreurs sur le test.  \n",
    ">\n",
    "> Vérifier que l'estimation de l'erreur ainsi obtenue est plus élevée que l'estimation obtenue précédemment, en ajustant et évaluant l'erreur sur le même échantillon (échantillon complet, sans découpage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9630032644178455\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nn = neighbors.KNeighborsClassifier()\n",
    "nn.fit(features_train, activity_train)\n",
    "print(nn.score(features_test, activity_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Reproduire la procédure 100 fois et dresser le boxplot de l'estimation de l'erreur sur ces 100 runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "score_train_test = [] # pour stockage des scores\n",
    "for simu in range(100) :\n",
    "    features_train, features_test, activity_train, activity_test = model_selection.train_test_split(activity_features, activity)\n",
    "    nn_train_test = neighbors.KNeighborsClassifier()\n",
    "    nn_train_test.fit(features_train, activity_train)\n",
    "    score_train_test.append(nn.score(features_test, activity_test))\n",
    "B = plt.boxplot(score_train_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Tracer la courbe d'erreur du classifieur par plus proches voisins en fonction du nombre de voisins utilisés (de 1 à 15), en utilisant cette méthode de découpage train / test répétée 20 fois."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En réalité, si l'on désire une estimation du modèle sélectionné ainsi, il faut découper l'échantillon disponible non pas en deux mais en trois parties :\n",
    "- l'ensemble d'apprentissage : utilisé pour construire des modèles ;\n",
    "- l'ensemble de validation : utilisé pour choisir un modèle ;\n",
    "- l'ensemble de test : utilisé pour évaluer les performances du modèle\n",
    "  finalement choisi.\n",
    "  \n",
    "Ceci est notamment important lorsque l'on souhaite comparer deux méthodes qui ont toutes les deux fait intervenir un réglage de paramètres (ici le nombre de voisins).\n",
    "\n",
    "> **Question bonus** (finir le tp avant de répondre à cette question) : Mettre en oeuvre cette méthode pour estimer l'erreur de généralisation du prédicteur knn pour lequel le nombre de voisins est choisi via l'échantillon de validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimation du risque par validation croisée \n",
    "\n",
    "On présente ci-dessous la méthode du de validation croisée de type **k-fold**.\n",
    "\n",
    "L'idée principale est de faire jouer à chaque observation à la fois le rôle de donnée d'apprentissage et celui de donnée de validation. \n",
    "\n",
    "Le principe consiste à partitionner aléatoirement les données $D=(\\mathbf{x}_i,y_i)_{1\\leq i\\leq N} $ \n",
    "en $K$ blocs disjoints de tailles comparables :\n",
    "$D^1,\\ldots,D^K$. On prend généralement $K=5$ ou $10$. La méthode du **leave-one-out (Loo)** correspond au choix $K=N$. \n",
    "\n",
    "On estime alors la qualité d'un predicteur $\\hat f$ de la\n",
    "façon suivante :\n",
    "- pour tout $k \\in \\{1,\\dots, K\\}$, on construit un predicteur $\\hat f^k$ (du même type que $\\hat f$) sur l'ensemble d'apprentissage $D^{-k} := D\\setminus D^k$ ;\n",
    "- on estime le risque du prédicteur $\\hat f$ par   \n",
    "$$\n",
    "\\frac{1}{N}\\sum_{k=1}^K\\sum_{\\mathbf{x}_i\\in D^k} \\ell(\\hat f^k(\\mathbf{x}_i),y_i),\n",
    "$$\n",
    "où $\\ell$ est la perte utilisée.\n",
    "\n",
    "On construit donc autant de prédicteurs qu'il y a de blocs dans\n",
    "l'ensemble d'apprentissage. Chaque prédicteur est évalué sur les éléments du\n",
    "bloc qui n'a pas été utilisé pour l'apprentissage (et qui joue donc le rôle d'\n",
    "ensemble de validation). \n",
    "\n",
    "Lorsque l'on souhaite régler un paramètre en utilisant la validation croisée, par exemple le nombre de voisins $k$ pour knn, on procède comme suit :\n",
    "- on évalue les performances des prédicteurs par validation croisée comme expliqué ci-dessus ;\n",
    "- on détermine la valeur optimale du paramètre ;\n",
    "- on construit finalement un predicteur pour le paramètre sélectionné, en utilisant comme base d'apprentissage toutes les données.\n",
    "\n",
    "\n",
    "**Comparaison entre LOO et methodes k folds, quelques repères :** \n",
    "- LOO a un coût computationnel plus élevé que les méthodes k-fold pour k petit (pour k = 5 ou 10 par exemple)\n",
    "- L'estimation de l'erreur fournie par LOO a généralement une variance plus élevée que celles obtenues par un k-fold pour k petit.\n",
    "- En revanche, si l'erreur de classification décroit très rapidement avec $n$, les méthodes k-fold avec k petit peuvent surestimer significativement l'erreur de généralisation.\n",
    "- En général, il est souvent recommandé d'utiliser les méthodes k-fold avec k = 5 ou 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'ensemble des méthodes de validation croisée disponibles sous `sklearn` sont disponibles dans le module [<code>model_selection</code>](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.model_selection). Voir aussi le [guide](https://scikit-learn.org/stable/modules/cross_validation.html#)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Estimation de l'erreur par 10-fold :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_val_croisee = neighbors.KNeighborsClassifier(n_neighbors = 10)\n",
    "scores = model_selection.cross_val_score(nn_val_croisee, features34, activity34, cv=10)\n",
    "print(scores)\n",
    "print(mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention cependant, avec cette méthode, la fonction `cross_val_score` ne permute pas préalablement les données avant de définir les blocs. Voir cette [note](https://scikit-learn.org/stable/modules/cross_validation.html#a-note-on-shuffling) dans la doc de sklearn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Modifier la procédure pour que les données soient bien initiallement permutées. On utilise pour cela un \"iterateur\" de validation croisée, que l'on définit ici avec la fonction `KFold`, et que l'on fournit en argument de la fonction `cross_val_score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_kfold = model_selection.KFold(### TO DO ###)\n",
    "\n",
    "nn_val_croisee = neighbors.KNeighborsClassifier(n_neighbors = 10)\n",
    "\n",
    "scores = model_selection.cross_val_score(estimator=### TO DO ###,\n",
    "                        X=### TO DO ###,\n",
    "                        y=### TO DO ###,\n",
    "                        cv=### TO DO ###,\n",
    "                        n_jobs=-1) # permet de répartir les calculs sur plusieurs coeurs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Estimation de l'erreur par Loo\n",
    "\n",
    "\n",
    "> Utiliser la fonction <code>LeaveOneOut</code> de `model_selection` pour évaluer l'erreur du classifieur de plus proches voisin (pour 10 voisins).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> La question de la permutation aléatoire initiale des données se pose-t-elle ici aussi ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sélection du nombre de plus proches voisins par validation croisée 10 fold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Selectionner un nombre de voisins pour le classifieur de plus proches voisins par validation croisée 10 fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La fonction GridSearchCV\n",
    "\n",
    "En apprentissage statistique, on a très souvent recours à la validation croisée pour régler des paramètres de la méthode utilisée. La fonction [gridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) permet d'effectuer cette tâche de façon simple, en répartissant éventuellement les calculs sur plusieurs coeurs (voir plus bas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from time import time\n",
    "\n",
    "# la grille de parametres a regler sont definis dans un dictionnaire (dict)\n",
    "tuned_parameters = {'n_neighbors': range(2,20)}\n",
    "start = time()\n",
    "\n",
    "my_kfold = model_selection.KFold(n_splits=10, shuffle=True, random_state=0)\n",
    "nnGrid = GridSearchCV(neighbors.KNeighborsClassifier(),\n",
    "                      tuned_parameters,\n",
    "                      cv=5)\n",
    "nnGrid.fit(features34, activity34)\n",
    "print(str(time() - start)+ \" sec\")\n",
    "\n",
    "# le meilleur modele \n",
    "print(nnGrid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Si votre machine le permet, utiliser maintenant plusieurs coeurs (argument \"n_jobs= \") et comparer les temps de calcul."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Classification multiclasses \n",
    "\n",
    "Pour réduire les temps de calcul (il s'agit d'un TP...) nous appliquons la même stratégie de présélection de features que nous avons utilisé pour la classification binaire."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Présélection de features\n",
    "\n",
    "Pour réduire les temps de calcul (il s'agit d'un TP...) nous appliquons la même stratégie de présélection de features que pour le cas à deux classes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Selec = SelectKBest(f_classif, k=100)\n",
    "features_all_activities = Selec.fit_transform(activity_features, activity)\n",
    "shape(features_all_activities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifieur naif bayesien multiclasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB()\n",
    "scores  = model_selection.cross_val_score(gnb, features_all_activities, activity, cv=my_kfold)\n",
    "print(scores)\n",
    "print(mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifieur des k plus proches voisins multiclasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = neighbors.KNeighborsClassifier(n_neighbors = 10)\n",
    "scores  = model_selection.cross_val_score(nn, features_all_activities, activity, cv=my_kfold)\n",
    "print(scores)\n",
    "print(mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Effectuer un découpage apprentissage / test des données (avec toutes les classes).\n",
    "> Utiliser la fonction `GridSearchCV` sur l'échantillon d'apprentissage pour choisir le nombre de voisins pour l'estimateur knn. Dresser et afficher la matrice de confusion sur les données de test, voir [ici](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) et [ici](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.plot_confusion_matrix.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression logistique Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Ajuster et évaluer un modèle logistique Lasso sur les données avec la fonction [LogisticRegressionCV](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Analyse en composantes principales sur la tableau des features.\n",
    "\n",
    "L'analyse en composantes principales est une méthode de réduction de dimension, de visualisation et de compression de données. Elle consiste à déterminer le sous-espace vectoriel de dimension $q$ qui approche au mieux possible un nuage de points dans $\\mathbb R^d$ (au sens de la projection euclidienne sur cet espace).\n",
    "\n",
    "Les directions du sous-espace vectoriel approximant peuvent être vues comme de nouvelles variables, qui s'éxpriment comme des combinaisons linéaires des variables initiales. Par ailleurs, ces nouvelles variables, appelées **composantes principales**, sont dirigées selon les premières directions propres de la matrice de covariance empirique des données.\n",
    "\n",
    "On représente alors les données dans le nouveau système de variables issu de l'ACP.  \n",
    "\n",
    "![analyse en composantes principales](http://www.nlpca.org/fig_pca_principal_component_analysis.png)\n",
    "\n",
    "Un descriptif de la fonction `sklearn.decomposition.PCA()` de la librairie scikit-learn est disponible [ici](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html).\n",
    "\n",
    "> Tracer le nuage des deux premières composantes de l'ACP en distinguant les 5 activités. \n",
    "> Dans son écriture, le problème de l'ACP ignore la classification en classes d'activité. Les classes d'activités sur ces deux premières composantes sont elles bien séparées ? \n",
    "> En particulier, vérifier graphiquement que les classes 3 et 4 (pour lesquelles nous avons proposé des classifieurs plus haut) ne sont pas facilement séparées par les deux premiers axes de l'APC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=5)\n",
    "print(pca)\n",
    "\n",
    "### TODO ###    # ajuster la PCA sur les données features_nor\n",
    "### TODO ###    # transformer (prediction) les données features_nor pour le modele PCA\n",
    "ACP0 = ### TODO ### extraction premiere composante\n",
    "ACP1 = ### TODO ### extraction deuxieme composante\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "for c, i, act in zip(\"rgbycm\", range(0,6),range(0,6)):\n",
    "    plt.scatter(### TODO ###)\n",
    "# iterer sur les activités et utiliser un zip pour\n",
    "# faciliter la representation graphique des objets \n",
    "# sur lesquels on itere\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "c951138b5c09beb1591d3d72231bb072894bfff4869d1694c99d11da1abe12fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
